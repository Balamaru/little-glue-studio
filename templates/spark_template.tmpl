from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Initialize Spark Session
spark = SparkSession.builder.appName("Generated ETL Job").getOrCreate()

# Define source data loading
{{if eq .SourceType "s3"}}
# Load data from Amazon S3
spark._jsc.hadoopConfiguration().set("fs.s3a.access.key", "{{.S3Source.AccessKey}}")
spark._jsc.hadoopConfiguration().set("fs.s3a.secret.key", "{{.S3Source.SecretKey}}")
df = spark.read.format("parquet").load("s3a://{{.S3Source.Bucket}}/{{.S3Source.Path}}")
{{else if eq .SourceType "s3_compatible"}}
# Load data from S3-compatible storage
spark._jsc.hadoopConfiguration().set("fs.s3a.access.key", "{{.S3Source.AccessKey}}")
spark._jsc.hadoopConfiguration().set("fs.s3a.secret.key", "{{.S3Source.SecretKey}}")
spark._jsc.hadoopConfiguration().set("fs.s3a.endpoint", "{{.S3Source.EndpointURL}}")
spark._jsc.hadoopConfiguration().set("fs.s3a.path.style.access", "true")
df = spark.read.format("parquet").load("s3a://{{.S3Source.Bucket}}/{{.S3Source.Path}}")
{{else if eq .SourceType "postgresql"}}
# Load data from PostgreSQL
df = spark.read.format("jdbc") \
    .option("url", "jdbc:postgresql://{{.PgSource.Host}}:{{.PgSource.Port}}/{{.PgSource.Database}}") \
    .option("dbtable", "{{.PgSource.Table}}") \
    .option("user", "{{.PgSource.User}}") \
    .option("password", "{{.PgSource.Password}}") \
    .load()
{{end}}

# Apply transforms
{{range $index, $transform := .Transforms}}
# Transform {{$index}}: {{$transform.Type}}
{{if eq $transform.Type "filter"}}
df = df.filter("{{$transform.Condition}}")
{{else if eq $transform.Type "select"}}
df = df.select({{range $i, $col := $transform.Columns}}{{if $i}}, {{end}}"{{$col}}"{{end}})
{{else if eq $transform.Type "rename_column"}}
{{range $old, $new := $transform.RenameColumns}}
df = df.withColumnRenamed("{{$old}}", "{{$new}}")
{{end}}
{{else if eq $transform.Type "drop_column"}}
df = df.drop({{range $i, $col := $transform.Columns}}{{if $i}}, {{end}}"{{$col}}"{{end}})
{{else if eq $transform.Type "add_column"}}
df = df.withColumn("{{$transform.Name}}", F.expr("{{$transform.Expression}}"))
{{else if eq $transform.Type "join"}}
# Load right dataframe for join
{{if eq $transform.RightSourceType "s3"}}
spark._jsc.hadoopConfiguration().set("fs.s3a.access.key", "{{$transform.RightS3Source.AccessKey}}")
spark._jsc.hadoopConfiguration().set("fs.s3a.secret.key", "{{$transform.RightS3Source.SecretKey}}")
right_df = spark.read.format("parquet").load("s3a://{{$transform.RightS3Source.Bucket}}/{{$transform.RightS3Source.Path}}")
{{else if eq $transform.RightSourceType "s3_compatible"}}
spark._jsc.hadoopConfiguration().set("fs.s3a.access.key", "{{$transform.RightS3Source.AccessKey}}")
spark._jsc.hadoopConfiguration().set("fs.s3a.secret.key", "{{$transform.RightS3Source.SecretKey}}")
spark._jsc.hadoopConfiguration().set("fs.s3a.endpoint", "{{$transform.RightS3Source.EndpointURL}}")
spark._jsc.hadoopConfiguration().set("fs.s3a.path.style.access", "true")
right_df = spark.read.format("parquet").load("s3a://{{$transform.RightS3Source.Bucket}}/{{$transform.RightS3Source.Path}}")
{{else if eq $transform.RightSourceType "postgresql"}}
right_df = spark.read.format("jdbc") \
    .option("url", "jdbc:postgresql://{{$transform.RightPgSource.Host}}:{{$transform.RightPgSource.Port}}/{{$transform.RightPgSource.Database}}") \
    .option("dbtable", "{{$transform.RightPgSource.Table}}") \
    .option("user", "{{$transform.RightPgSource.User}}") \
    .option("password", "{{$transform.RightPgSource.Password}}") \
    .load()
{{end}}

# Create join conditions
join_condition = {{range $i, $lcol := $transform.LeftColumns}}{{$rcol := index $transform.RightColumns $i}}{{if $i}} & {{end}}(df["{{$lcol}}"] == right_df["{{$rcol}}"]){{end}}

# Apply join
df = df.join(right_df, join_condition, "{{$transform.JoinType}}")
{{else if eq $transform.Type "groupby_agg"}}
# Group by and aggregate
df = df.groupBy({{range $i, $col := $transform.GroupByCols}}{{if $i}}, {{end}}"{{$col}}"{{end}}) \
    .agg(
        {{range $i, $col := $transform.AggregationColumns}}
        {{if $i}},
        {{end}}F.{{index $transform.AggregationFunctions $i}}("{{$col}}").alias("{{$col}}_{{index $transform.AggregationFunctions $i}}")
        {{end}}
    )
{{end}}

{{end}}

# Write to target destination
{{if eq .Target.Type "s3"}}
# Write to Amazon S3
spark._jsc.hadoopConfiguration().set("fs.s3a.access.key", "{{.Target.AccessKey}}")
spark._jsc.hadoopConfiguration().set("fs.s3a.secret.key", "{{.Target.SecretKey}}")
df.write.mode("overwrite").parquet("s3a://{{.Target.Bucket}}/{{.Target.Path}}")
{{else if eq .Target.Type "s3_compatible"}}
# Write to S3-compatible storage
spark._jsc.hadoopConfiguration().set("fs.s3a.access.key", "{{.Target.AccessKey}}")
spark._jsc.hadoopConfiguration().set("fs.s3a.secret.key", "{{.Target.SecretKey}}")
spark._jsc.hadoopConfiguration().set("fs.s3a.endpoint", "{{.Target.EndpointURL}}")
spark._jsc.hadoopConfiguration().set("fs.s3a.path.style.access", "true")
df.write.mode("overwrite").parquet("s3a://{{.Target.Bucket}}/{{.Target.Path}}")
{{else if eq .Target.Type "postgresql"}}
# Write to PostgreSQL
df.write \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://{{.Target.Host}}:{{.Target.Port}}/{{.Target.Database}}") \
    .option("dbtable", "{{.Target.Table}}") \
    .option("user", "{{.Target.User}}") \
    .option("password", "{{.Target.Password}}") \
    .mode("overwrite") \
    .save()
{{end}}

print("ETL job completed successfully!")